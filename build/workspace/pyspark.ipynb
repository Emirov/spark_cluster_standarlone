{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PySpark**: L'api de Apache Sparck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Ce notebook nous montre comment ce connecter au cluster de spark et faire un petit ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Connexion\n",
    "\n",
    "Pour ce connecter au spark cluster on utliser \n",
    "+ **appName:** Le nome de l'app affiché dans l'UI [Spark Master Web UI](http://localhost:8080/);\n",
    "+ **master:** L'URL du master la meme utilisé par les workers\n",
    "+ **spark.executor.memory:** doit etre égale ou supérieur SPARK_WORKER_MEMORY dans docker compose config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ajouter plus de config à SparkSession object dans le utilisant la méthode **config**. Voir API docs [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Introduction\n",
    "On va Spark Python API to lire, procéder and écrire la data. Voir API docs [here](https://spark.apache.org/docs/latest/api/python/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Read\n",
    "\n",
    "On va lire UK's macroeconomic data ([source](https://www.kaggle.com/bank-of-england/a-millennium-of-macroeconomic-data)) du cluster simulé  **Hadoop distributed file system (HDFS)** dans Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(path=\"data/uk-macroeconomic-data.csv\", sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va affcicher dataframe metadata, comme le nombre de colonnes les lignes et le shéma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "841"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va tout d'abord séléctionner les data pertinents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = data.select([\"Description\", \"Population (GB+NI)\", \"Unemployment rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----------------+\n",
      "|Description|Population (GB+NI)|Unemployment rate|\n",
      "+-----------+------------------+-----------------+\n",
      "|      Units|              000s|                %|\n",
      "|       1209|              null|             null|\n",
      "|       1210|              null|             null|\n",
      "|       1211|              null|             null|\n",
      "|       1212|              null|             null|\n",
      "|       1213|              null|             null|\n",
      "|       1214|              null|             null|\n",
      "|       1215|              null|             null|\n",
      "|       1216|              null|             null|\n",
      "|       1217|              null|             null|\n",
      "+-----------+------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unemployment.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a séléctionné avec succès les colonnes requises et afficher les 10 premiers colonnes mais on a un problème:\n",
    "+ les prmières lignes ne contiennent pas de la data mais les unité de mesure;\n",
    "+ There are many years with missing population and unemployment data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va supprimer la première ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_description = unemployment.filter(unemployment['Description'] == 'Units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----------------+\n",
      "|Description|Population (GB+NI)|Unemployment rate|\n",
      "+-----------+------------------+-----------------+\n",
      "|      Units|              000s|                %|\n",
      "+-----------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_description.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = unemployment.join(other=cols_description, on=['Description'], how='left_anti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----------------+\n",
      "|Description|Population (GB+NI)|Unemployment rate|\n",
      "+-----------+------------------+-----------------+\n",
      "|       1209|              null|             null|\n",
      "|       1210|              null|             null|\n",
      "|       1211|              null|             null|\n",
      "|       1212|              null|             null|\n",
      "|       1213|              null|             null|\n",
      "|       1214|              null|             null|\n",
      "|       1215|              null|             null|\n",
      "|       1216|              null|             null|\n",
      "|       1217|              null|             null|\n",
      "|       1218|              null|             null|\n",
      "+-----------+------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unemployment.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now, let's drop the dataframe rows with missing data and refactor its columns names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = unemployment.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = unemployment.\\\n",
    "               withColumnRenamed(\"Description\", 'year').\\\n",
    "               withColumnRenamed(\"Population (GB+NI)\", \"population\").\\\n",
    "               withColumnRenamed(\"Unemployment rate\", \"unemployment_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------------+\n",
      "|year|population|unemployment_rate|\n",
      "+----+----------+-----------------+\n",
      "|1855|     23241|             3.73|\n",
      "|1856|     23466|             3.52|\n",
      "|1857|     23689|             3.95|\n",
      "|1858|     23914|             5.23|\n",
      "|1859|     24138|             3.27|\n",
      "|1860|     24360|             2.94|\n",
      "|1861|     24585|             3.72|\n",
      "|1862|     24862|             4.68|\n",
      "|1863|     25142|             4.15|\n",
      "|1864|     25425|             2.99|\n",
      "+----+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unemployment.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dernière chose on écrit la data dans notre HDFS simulé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment.repartition(1).write.csv(path=\"data/uk-macroeconomic-unemployment-data.csv\", sep=\",\", header=True, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
